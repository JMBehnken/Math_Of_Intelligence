{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137624 chars, 80 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l': 0, '0': 1, 'X': 2, 'm': 3, 'u': 4, '2': 5, 'T': 6, '7': 7, '5': 8, '%': 9, '@': 10, '(': 11, '*': 12, 'S': 13, 'z': 14, '6': 15, 'J': 16, '9': 17, 'F': 18, 'B': 19, 'W': 20, '4': 21, 'r': 22, 'o': 23, 't': 24, 'q': 25, '!': 26, ':': 27, 'y': 28, '\"': 29, 'L': 30, 'C': 31, 'O': 32, '.': 33, 'Y': 34, ',': 35, 'H': 36, 's': 37, '-': 38, 'x': 39, 'p': 40, 'P': 41, \"'\": 42, 'G': 43, 'E': 44, 'M': 45, 'U': 46, 'n': 47, 'e': 48, 'k': 49, 'v': 50, 'R': 51, 'h': 52, 'I': 53, 'a': 54, 'f': 55, 'c': 56, 'V': 57, '/': 58, '8': 59, '\\n': 60, 'K': 61, 'w': 62, 'D': 63, ' ': 64, 'j': 65, 'N': 66, 'g': 67, 'A': 68, 'Q': 69, '$': 70, 'd': 71, ')': 72, 'b': 73, '3': 74, 'รง': 75, 'i': 76, ';': 77, '1': 78, '?': 79}\n",
      "{0: 'l', 1: '0', 2: 'X', 3: 'm', 4: 'u', 5: '2', 6: 'T', 7: '7', 8: '5', 9: '%', 10: '@', 11: '(', 12: '*', 13: 'S', 14: 'z', 15: '6', 16: 'J', 17: '9', 18: 'F', 19: 'B', 20: 'W', 21: '4', 22: 'r', 23: 'o', 24: 't', 25: 'q', 26: '!', 27: ':', 28: 'y', 29: '\"', 30: 'L', 31: 'C', 32: 'O', 33: '.', 34: 'Y', 35: ',', 36: 'H', 37: 's', 38: '-', 39: 'x', 40: 'p', 41: 'P', 42: \"'\", 43: 'G', 44: 'E', 45: 'M', 46: 'U', 47: 'n', 48: 'e', 49: 'k', 50: 'v', 51: 'R', 52: 'h', 53: 'I', 54: 'a', 55: 'f', 56: 'c', 57: 'V', 58: '/', 59: '8', 60: '\\n', 61: 'K', 62: 'w', 63: 'D', 64: ' ', 65: 'j', 66: 'N', 67: 'g', 68: 'A', 69: 'Q', 70: '$', 71: 'd', 72: ')', 73: 'b', 74: '3', 75: 'รง', 76: 'i', 77: ';', 78: '1', 79: '?'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model parameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "  #store our inputs, hidden states, outputs, and probability values\n",
    "  xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  #init loss as 0\n",
    "  loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards    \n",
    "  #initalize vectors for gradient values for each set of weights \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #output probabilities\n",
    "    dy = np.copy(ps[t])\n",
    "    #derive our first gradient\n",
    "    dy[targets[t]] -= 1 # backprop into y  \n",
    "    #compute output gradient -  output times hidden states transpose\n",
    "    #When we apply the transpose weight matrix,  \n",
    "    #we can think intuitively of this as moving the error backward\n",
    "    #through the network, giving us some sort of measure of the error \n",
    "    #at the output of the lth layer. \n",
    "    #output gradient\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    #derivative of output bias\n",
    "    dby += dy\n",
    "    #backpropagate!\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "    dbh += dhraw #derivative of hidden bias\n",
    "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "    dhnext = np.dot(Whh.T, dhraw) \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " c$(kxs3Xf$OJ$*f/vYUST@pUmbq9w!$x37e/;\n",
      "WcfBkVlq?8OX \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step   \n",
    "  n is how many characters to predict\n",
    "  \"\"\"\n",
    "  #create vector\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  #customize it for our seed char\n",
    "  x[seed_ix] = 1\n",
    "  #list to store generated chars\n",
    "  ixes = []\n",
    "  #for as many characters as we want to generate\n",
    "  for t in range(n):\n",
    "    #a hidden state at a given time step is a function \n",
    "    #of the input at the same time step modified by a weight matrix \n",
    "    #added to the hidden state of the previous time step \n",
    "    #multiplied by its own hidden state to hidden state matrix.\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    #compute output (unnormalised)\n",
    "    y = np.dot(Why, h) + by\n",
    "    ## probabilities for next chars\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #pick one with the highest probability \n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    #create a vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for the predicted char\n",
    "    x[ix] = 1\n",
    "    #add it to the list\n",
    "    ixes.append(ix)\n",
    "\n",
    "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "  print('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [32, 47, 48, 64, 3, 23, 22, 47, 76, 47, 67, 35, 64, 62, 52, 48, 47, 64, 43, 22, 48, 67, 23, 22, 64]\n",
      "targets [47, 48, 64, 3, 23, 22, 47, 76, 47, 67, 35, 64, 62, 52, 48, 47, 64, 43, 22, 48, 67, 23, 22, 64, 13]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print(\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print(\"targets\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 109.550670\n",
      "----\n",
      " D)PA\n",
      "wryvcj4u/AfoKBKyX3m.'6W/0 ,DT70E\n",
      "u7kq5HN*iO'r \n",
      "----\n",
      "iter 1000, loss: 85.405387\n",
      "----\n",
      "  wapthe, ase is gid hemeesy the.  vide rheyke winl \n",
      "----\n",
      "iter 2000, loss: 68.977607\n",
      "----\n",
      " lthe the lol, ,oohere jetharrl he tn\"kas ron -he s \n",
      "----\n",
      "iter 3000, loss: 60.775373\n",
      "----\n",
      " e shalge cinge merocithefred ik. crifon, thy wad f \n",
      "----\n",
      "iter 4000, loss: 56.615702\n",
      "----\n",
      "  hes ordyr Voryor si!etore, int ohpeg the ulr ivs  \n",
      "----\n",
      "iter 5000, loss: 59.539478\n",
      "----\n",
      " wing cnithid onoly; the th pasp; withit this ouler \n",
      "----\n",
      "iter 6000, loss: 60.948190\n",
      "----\n",
      " y vouldont lell ave fe, sawas to bo nt ther an es  \n",
      "----\n",
      "iter 7000, loss: 56.350287\n",
      "----\n",
      " n was his tay taknwis he the ha tentrun: moshatft  \n",
      "----\n",
      "iter 8000, loss: 52.939898\n",
      "----\n",
      " mly d's capsist famey ntout ecurf tos cohe at has  \n",
      "----\n",
      "iter 9000, loss: 51.172342\n",
      "----\n",
      "  thouadioned the and tha fooln mum has -tored than \n",
      "----\n",
      "iter 10000, loss: 50.422570\n",
      "----\n",
      " hed wir ther ho doniched ristut the wastowet if wi \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "while n<=100*100:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 1000 == 0:\n",
    "    print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    sample(hprev, inputs[0], 50)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_length # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ing.\n",
      "\n",
      "Tren the difing sackley anlenes erely., com? hid on wed wer took is sto hee horented thas hise noor. Mr gooe cale ald in ham, the then pmeas - cefrice Gregor uaning hak the in thawibldestas domeedent. Ote be-ik the in - ain souns frich siiger\"\n",
      "\n",
      "The asto has proucladivecone marky shad ap the wor toot him natt it where theikad if the deon of the susay sade whid at aad thell was therter, en picked mo had for hiy the reesit Dathding say team; and do the eser lasoede, lreanlm on ift hed dregozst ald wite pust wangthe cofh ther to shoved cher in woughad notlitt sad hit the wine dor moo kile do port leme wouved the moat to his whipn disiln han's it was, \"o hared stemald aly toe nongl hes wand daints thete suiclease ving meean's wolw exwibellernwo the - theaf thas voows. dimtels, the waveamow; Gmegot didinge If? I cur the to theb ink and ove preitgut wory the hemr bome as eftat lene the loed the wey htars steven, hand gonkl oney eis thor, an legubln. Bud the one \"\"or frhaw onisty was'd nother morast nmathe he was this ther ihe no toot monllater and af ther soit and cork herenge, vamed was \"The preaf hande to himld an sers yove cald con shexpled hid in ate mous, baga sivern, moth teed wim., \"So bent juld fhered I casging lotplen whok catring had momst if seghem ingagor aby the noikked frors lpeemrilet,'m. TEvea lepl opsher's ond on the wistinist\n",
      "ong so wof sad, hink stio mLGrokes armed, gor. -s as on in the 'steg in soothed favem. Sushe he to saik, noo at andered fasw slail to ant at wis iniwo anfy\n",
      ".o the rate cot\" at. Dnevurlive theors oundpalstpant quif lacker non do rrestr fowd that hir hed tockrss onghlned and wame anst it mat oupledey the anat dow, simey coon at tr swad the dianising shed and intthere vetlit she thet is then. R; In aand, kel, Sor pad the rourdent her frsiece oprte mo kizd. \"houifolly wemee wo perrey whort of to momd them ther gom'ss aidd tienomes, on to lestored wayc. Hitely istunt uryldo was worlew, slany if shat tisteshithyy and yoor'de fome aydriro do chre sith thas fart if ridey.. Gregor - bust, and rilg thouldiny wad, siad pwandipse mofetansbom, hid'd if withe fork the ffor al? The cat hed tied, and in sile the wead f ithith pinosnt and ot lecisleg ened blwed mooit an atexpedinglany, notis in post Af shat it plowiwas in ther his. Gr potr nouk hemy fromome hime notnecthit she were sieneln to with ho kasted wak'sy Gregomiy the court, cofe dow tieg sald dops Gregoly.\"\"\n",
      "Thishis moefwe to and the silt apkey arlly the heist lieny ockist the hay tos foon \"it she heely breclled appion thet on to and oublint.\n",
      "An ve the ofcatr had an fongletharn. An the weoms qure luling and. Shea sith acuint wily \"o s if chaken. An the vey trother ole ther latl gom cragas in seever, therdy fatted of sheersselly the cont'efn fro prat juke ag hied the ffwung wast woule ape frousrenitl owenfand bpeinh om sulltht in: \")es nof, shomame, d\"torly ther thas apred that the rowt of had atlistsey be mo of dithe waker frad wing the war thate bathr graaped rably Mrithered if famoen then whick at woud the wank ta tomisester frerepin, stor apel bike ina asengan mats. I fangithis \", sharle Gregardis eveips be im weltor roor istont is laded ugred had slint. go at and then assriger thainllimed, reent. Iil fand ratarl if ard. ke apsenciplightll oped, the veeats atay hit loos. Greor begoow Grof hay sigat, snookby rubded and of and zimped beeth he angrast teer wiig and htrreld vey wer in\" his sersed. lingor jome, arsing tregst Gregay, byer the 'therole thele the und ther, lighs tore, not and wowhar unos stqegood sricler? shor, abkissurd wis whag tood aprse the ther, fumpuckmory thed sad sioh wharled whe pregeing cor. Had and abd ton to ink net ald the morod areft stoom ous sinc., orle. \"ave mirs selivathes for to his sircevooH Mock the bit sher ahat dord, thtreing lore padt exsped ild warde thy laay uine oet and neo the bement, the uk of arver himt avot tirt hats chooned ant het in her lele hters hand wistim awiling; mat more wa ling and wsint the nom sumiind rad rursisten Gregistinn was frere the t\"'th im ofvey and siand and way thlore she gored sheytor turred woure hire is harw he ror eut ind to and thet the dor sedtild erdigked erring Gryhart taFt Mrigntuld sars allan thair to chat as sigto weo habicked ot souwt Gregover even alpebt enon tor celild the mrooms aver, ind, no in, apsent. 1leny what and dowters hed momivir. gxern had asor the seding ther'ng che morh ot but aibss ile to himp sad, dom to tof lopecl actanebiDs dyred led. Hew lissed aby ify. \" me some the mamy, his is rongithed hed. Theine to soom meiffousand his ar. Hat it herey not hit loudpel ward. Sall; utireytand the roor the bithet cuupligh fured tharked ray hed, har heerree, hand stien womeenh is fileld dowe his bpeot -oem aland the ding goer of the enow, mo kaghomer thea boinsaply owem meicoot wive the vime aivens llourpint imhivstly into aick.\n",
      "\n",
      "Enquwen thes kich they hat ritt wamsed a denthe thing; bage layd out. gake. Supe hes aing.\"\n",
      "Com ant on ond youth it  \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "sample(hprev, inputs[0], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
