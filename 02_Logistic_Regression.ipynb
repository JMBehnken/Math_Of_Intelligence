{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#matrix math\n",
    "import numpy as np\n",
    "#data manipulation\n",
    "import pandas as pd\n",
    "#matrix data structure\n",
    "from patsy import dmatrices\n",
    "#for error logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#outputs probability between 0 and 1, used to help define our logistic regression curve\n",
    "def sigmoid(x):\n",
    "    '''Sigmoid function of x.'''\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes the random numbers predictable\n",
    "#(pseudo-)random numbers work by starting with a number (the seed), \n",
    "#multiplying it by a large number, then taking modulo of that product. \n",
    "#The resulting number is then used as the seed to generate the next \"random\" number. \n",
    "#When you set the seed (every time), it does the same thing every time, giving you the same numbers.\n",
    "#good for reproducing results for debugging\n",
    "\n",
    "\n",
    "np.random.seed(0) # set the seed\n",
    "\n",
    "##Step 1 - Define model parameters (hyperparameters)\n",
    "\n",
    "## algorithm settings\n",
    "#the minimum threshold for the difference between the predicted output and the actual output\n",
    "#this tells our model when to stop learning, when our prediction capability is good enough\n",
    "tol=1e-8 # convergence tolerance\n",
    "\n",
    "lam = None # l2-regularization\n",
    "#how long to train for?\n",
    "max_iter = 20 # maximum allowed iterations\n",
    "\n",
    "## data creation settings\n",
    "#Covariance measures how two variables move together. \n",
    "#It measures whether the two move in the same direction (a positive covariance) \n",
    "#or in opposite directions (a negative covariance). \n",
    "r = 0.95 # covariance between x and z\n",
    "n = 1000 # number of observations (size of dataset to generate) \n",
    "sigma = 1 # variance of noise - how spread out is the data?\n",
    "\n",
    "## model settings\n",
    "beta_x, beta_z, beta_v = -4, .9, 1 # true beta coefficients\n",
    "var_x, var_z, var_v = 1, 1, 4 # variances of inputs\n",
    "\n",
    "## the model specification you want to fit\n",
    "formula = 'y ~ x + z + v + np.exp(x) + I(v**2 + z)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jm/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>x</th>\n",
       "      <th>z</th>\n",
       "      <th>v</th>\n",
       "      <th>np.exp(x)</th>\n",
       "      <th>I(v ** 2 + z)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.805133</td>\n",
       "      <td>-1.678592</td>\n",
       "      <td>-230.536312</td>\n",
       "      <td>0.164453</td>\n",
       "      <td>53145.312449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.320743</td>\n",
       "      <td>-0.612110</td>\n",
       "      <td>-321.120883</td>\n",
       "      <td>0.266937</td>\n",
       "      <td>103118.009125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.689545</td>\n",
       "      <td>-1.998587</td>\n",
       "      <td>0.006285</td>\n",
       "      <td>0.184604</td>\n",
       "      <td>-1.998547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.914205</td>\n",
       "      <td>-0.962069</td>\n",
       "      <td>-56.335960</td>\n",
       "      <td>0.400835</td>\n",
       "      <td>3172.778273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036999</td>\n",
       "      <td>0.166842</td>\n",
       "      <td>-0.033775</td>\n",
       "      <td>1.037692</td>\n",
       "      <td>0.167983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.372172</td>\n",
       "      <td>0.087709</td>\n",
       "      <td>-22.317063</td>\n",
       "      <td>0.689235</td>\n",
       "      <td>498.139023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.770703</td>\n",
       "      <td>-0.732226</td>\n",
       "      <td>-29.307485</td>\n",
       "      <td>0.462688</td>\n",
       "      <td>858.196466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.491038</td>\n",
       "      <td>-0.385521</td>\n",
       "      <td>-7.115349</td>\n",
       "      <td>0.611991</td>\n",
       "      <td>50.242671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.442847</td>\n",
       "      <td>-1.507723</td>\n",
       "      <td>22.291060</td>\n",
       "      <td>0.236254</td>\n",
       "      <td>495.383624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.174085</td>\n",
       "      <td>-0.444174</td>\n",
       "      <td>51.337021</td>\n",
       "      <td>0.840225</td>\n",
       "      <td>2635.045551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intercept         x         z           v  np.exp(x)  I(v ** 2 + z)\n",
       "0        1.0 -1.805133 -1.678592 -230.536312   0.164453   53145.312449\n",
       "1        1.0 -1.320743 -0.612110 -321.120883   0.266937  103118.009125\n",
       "2        1.0 -1.689545 -1.998587    0.006285   0.184604      -1.998547\n",
       "3        1.0 -0.914205 -0.962069  -56.335960   0.400835    3172.778273\n",
       "4        1.0  0.036999  0.166842   -0.033775   1.037692       0.167983\n",
       "5        1.0 -0.372172  0.087709  -22.317063   0.689235     498.139023\n",
       "6        1.0 -0.770703 -0.732226  -29.307485   0.462688     858.196466\n",
       "7        1.0 -0.491038 -0.385521   -7.115349   0.611991      50.242671\n",
       "8        1.0 -1.442847 -1.507723   22.291060   0.236254     495.383624\n",
       "9        1.0 -0.174085 -0.444174   51.337021   0.840225    2635.045551"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 2 - Generate and organize our data\n",
    "\n",
    "#The multivariate normal, multinormal or Gaussian distribution is a generalization of the one-dimensional normal \n",
    "#distribution to higher dimensions. Such a distribution is specified by its mean and covariance matrix.\n",
    "#so we generate values input values - (x, v, z) using normal distributions\n",
    "\n",
    "#A probability distribution is a function that provides us the probabilities of all \n",
    "#possible outcomes of a stochastic process. \n",
    "\n",
    "#lets keep x and z closely related (height and weight)\n",
    "x, z = np.random.multivariate_normal([0,0], [[var_x,r],[r,var_z]], n).T\n",
    "#blood presure\n",
    "v = np.random.normal(0,var_v,n)**3\n",
    "\n",
    "#create a pandas dataframe (easily parseable object for manipulation)\n",
    "A = pd.DataFrame({'x' : x, 'z' : z, 'v' : v})\n",
    "#compute the log odds for our 3 independent variables\n",
    "#using the sigmoid function \n",
    "A['log_odds'] = sigmoid(A[['x','z','v']].dot([beta_x,beta_z,beta_v]) + sigma*np.random.normal(0,1,n))\n",
    "\n",
    "\n",
    "\n",
    "#compute the probability sample from binomial distribution\n",
    "#A binomial random variable is the number of successes x has in n repeated trials of a binomial experiment. \n",
    "#The probability distribution of a binomial random variable is called a binomial distribution. \n",
    "A['y'] = [np.random.binomial(1,p) for p in A.log_odds]\n",
    "\n",
    "#create a dataframe that encompasses our input data, model formula, and outputs\n",
    "y, X = dmatrices(formula, A, return_type='dataframe')\n",
    "\n",
    "#print it\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#like dividing by zero (Wtff omgggggg universe collapses)\n",
    "def catch_singularity(f):\n",
    "    '''Silences LinAlg Errors and throws a warning instead.'''\n",
    "    \n",
    "    def silencer(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except np.linalg.LinAlgError:\n",
    "            warnings.warn('Algorithm terminated - singular Hessian!')\n",
    "            return args[0]\n",
    "    return silencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@catch_singularity\n",
    "def newton_step(curr, X, lam=None):\n",
    "    '''One naive step of Newton's Method'''\n",
    "    \n",
    "    #how to compute inverse? http://www.mathwarehouse.com/algebra/matrix/images/square-matrix/inverse-matrix.gif\n",
    "    \n",
    "    ## compute necessary objects\n",
    "    #create probability matrix, miniminum 2 dimensions, tranpose (flip it)\n",
    "    p = np.array(sigmoid(X.dot(curr[:,0])), ndmin=2).T\n",
    "    #create weight matrix from it\n",
    "    W = np.diag((p*(1-p))[:,0])\n",
    "    #derive the hessian \n",
    "    hessian = X.T.dot(W).dot(X)\n",
    "    #derive the gradient\n",
    "    grad = X.T.dot(y-p)\n",
    "    \n",
    "    ## regularization step (avoiding overfitting)\n",
    "    if lam:\n",
    "        # Return the least-squares solution to a linear matrix equation\n",
    "        step, *_ = np.linalg.lstsq(hessian + lam*np.eye(curr.shape[0]), grad)\n",
    "    else:\n",
    "        step, *_ = np.linalg.lstsq(hessian, grad)\n",
    "        \n",
    "    ## update our \n",
    "    beta = curr + step\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@catch_singularity\n",
    "def alt_newton_step(curr, X, lam=None):\n",
    "    '''One naive step of Newton's Method'''\n",
    "    \n",
    "    ## compute necessary objects\n",
    "    p = np.array(sigmoid(X.dot(curr[:,0])), ndmin=2).T\n",
    "    W = np.diag((p*(1-p))[:,0])\n",
    "    hessian = X.T.dot(W).dot(X)\n",
    "    grad = X.T.dot(y-p)\n",
    "    \n",
    "    ## regularization\n",
    "    if lam:\n",
    "        #Compute the inverse of a matrix.\n",
    "        step = np.dot(np.linalg.inv(hessian + lam*np.eye(curr.shape[0])), grad)\n",
    "    else:\n",
    "        step = np.dot(np.linalg.inv(hessian), grad)\n",
    "        \n",
    "    ## update our weights\n",
    "    beta = curr + step\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_coefs_convergence(beta_old, beta_new, tol, iters):\n",
    "    '''Checks whether the coefficients have converged in the l-infinity norm.\n",
    "    Returns True if they have converged, False otherwise.'''\n",
    "    #calculate the change in the coefficients\n",
    "    coef_change = np.abs(beta_old - beta_new)\n",
    "    \n",
    "    #if change hasn't reached the threshold and we have more iterations to go, keep training\n",
    "    return not (np.any(coef_change>tol) & (iters < max_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations : 18\n",
      "Beta : [[ 26321107.15862302]\n",
      " [ -6345591.63389166]\n",
      " [-11270742.81176545]\n",
      " [ -9577260.73014144]\n",
      " [ 20672236.15494188]\n",
      " [ -7783630.58195644]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jm/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "## initial conditions\n",
    "#initial coefficients (weight values), 2 copies, we'll update one\n",
    "beta_old, beta = np.ones((len(X.columns),1)), np.zeros((len(X.columns),1))\n",
    "\n",
    "#num iterations we've done so far\n",
    "iter_count = 0\n",
    "#have we reached convergence?\n",
    "coefs_converged = False\n",
    "\n",
    "#if we haven't reached convergence... (training step)\n",
    "while not coefs_converged:\n",
    "    \n",
    "    #set the old coefficients to our current\n",
    "    beta_old = beta\n",
    "    #perform a single step of newton's optimization on our data, set our updated beta values\n",
    "    beta = newton_step(beta, X, lam=lam)\n",
    "    #increment the number of iterations\n",
    "    iter_count += 1\n",
    "    \n",
    "    #check for convergence between our old and new beta values\n",
    "    coefs_converged = check_coefs_convergence(beta_old, beta, tol, iter_count)\n",
    "    \n",
    "print('Iterations : {}'.format(iter_count))\n",
    "print('Beta : {}'.format(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
